{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvql6Bn9GUzi",
        "outputId": "42a5ff62-a97a-4286-88f2-b80dfe9d565b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6810 - loss: 0.8587 - val_accuracy: 0.9043 - val_loss: 0.2853\n",
            "Epoch 2/5\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9348 - loss: 0.2247 - val_accuracy: 0.9472 - val_loss: 0.1322\n",
            "Epoch 3/5\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9422 - loss: 0.1311 - val_accuracy: 0.9554 - val_loss: 0.1031\n",
            "Epoch 4/5\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9479 - loss: 0.1050 - val_accuracy: 0.9604 - val_loss: 0.0871\n",
            "Epoch 5/5\n",
            "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.0913 - val_accuracy: 0.9620 - val_loss: 0.0805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model training complete and saved!\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "üß™ Test Set Evaluation:\n",
            "‚úÖ Accuracy: 0.9617\n",
            "\n",
            "üìä Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "  Colon_Cancer       0.95      0.94      0.94       517\n",
            "   Lung_Cancer       1.00      1.00      1.00       407\n",
            "Thyroid_Cancer       0.95      0.95      0.95       590\n",
            "\n",
            "      accuracy                           0.96      1514\n",
            "     macro avg       0.97      0.96      0.96      1514\n",
            "  weighted avg       0.96      0.96      0.96      1514\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and clean data\n",
        "df = pd.read_csv(\"alldata_1_for_kaggle.csv\", header=None, encoding='ISO-8859-1')\n",
        "df.columns = ['index', 'label', 'text']\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Step 2: Remove rare labels (optional, but fixes sparse issues)\n",
        "label_counts = df['label'].value_counts()\n",
        "valid_labels = label_counts[label_counts >= 5].index\n",
        "df_filtered = df[df['label'].isin(valid_labels)].copy()\n",
        "\n",
        "# Step 3: Encode labels after filtering\n",
        "le = LabelEncoder()\n",
        "df_filtered['encoded_label'] = le.fit_transform(df_filtered['label'])\n",
        "\n",
        "# Save the encoder\n",
        "joblib.dump(le, \"label_encoder.joblib\")\n",
        "\n",
        "# Step 4: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_filtered['clean_text'], df_filtered['encoded_label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Save the vectorizer\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\n",
        "\n",
        "# Step 6: Model\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(X_train_tfidf.shape[1],)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train\n",
        "model.fit(X_train_tfidf.toarray(), y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Step 8: Save model\n",
        "model.save(\"medical_text_classifier.h5\")\n",
        "print(\"‚úÖ Model training complete and saved!\")\n",
        "\n",
        "# Step 9: Predict on test set\n",
        "y_pred_probs = model.predict(X_test_tfidf.toarray())\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Step 10: Evaluation\n",
        "print(\"\\nüß™ Test Set Evaluation:\")\n",
        "print(f\"‚úÖ Accuracy: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
        "print(\"üìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qNFyid7UrNm",
        "outputId": "be58525b-2f31-45ca-c21e-35817bf33210"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.9 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "CORSMiddleware,\n",
        "allow_origins=['*'],\n",
        "allow_credentials=True,\n",
        "allow_methods=['*'],\n",
        "allow_headers=['*'],\n",
        ")\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load saved artifacts\n",
        "model = tf.keras.models.load_model(\"medical_text_classifier.h5\")\n",
        "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.joblib\")\n",
        "label_encoder = joblib.load(\"label_encoder.joblib\")\n",
        "\n",
        "\n",
        "# Pydantic model for request body\n",
        "class TextInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Predict endpoint\n",
        "@app.post(\"/predict\")\n",
        "def predict(input: TextInput):\n",
        "    try:\n",
        "        cleaned = clean_text(input.text)\n",
        "        vectorized = tfidf_vectorizer.transform([cleaned])\n",
        "        prediction_probs = model.predict(vectorized.toarray())\n",
        "        predicted_index = np.argmax(prediction_probs, axis=1)[0]\n",
        "        predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
        "        confidence = float(np.max(prediction_probs))\n",
        "\n",
        "        return {\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"confidence\": round(confidence, 4)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf9kfAUjZ0SK",
        "outputId": "4c78d342-c389-4165-9f6f-62de8265b141"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5K5WV0tMgiNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "auth_token = \"2xmaGUkgfunyh5duVClAtxhx3Jh_3smPzAHYvWVLsdH7oT61F\"\n",
        "\n",
        "# Set the authtoken\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "# Connect to ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "\n",
        "# Print the public URL\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the uvicorn server\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyRgc67db2l0",
        "outputId": "53432ab3-f491-43dd-a319-4cc0546518e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.9)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Public URL: https://566d-34-141-220-78.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [758]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:9273:36d6:518b:5901:f991:3c87:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:9273:36d6:518b:5901:f991:3c87:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2401:4900:9273:36d6:518b:5901:f991:3c87:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:9273:36d6:518b:5901:f991:3c87:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
            "INFO:     2401:4900:9273:36d6:518b:5901:f991:3c87:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIGuuCMmcBJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}